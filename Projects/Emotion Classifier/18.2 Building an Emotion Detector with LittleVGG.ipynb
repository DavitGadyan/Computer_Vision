{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LittleVGG for Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Emotion Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "num_classes = 6\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 16\n",
    "\n",
    "train_data_dir = './fer2013/train'\n",
    "validation_data_dir = './fer2013/validation'\n",
    "\n",
    "# Let's use some data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import ELU\n",
    "from tensorflow.keras.layers import Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LittleVGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 64)        640       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 256)       295168    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 256)       590080    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 256)       1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               2359808   \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 7,120,710\n",
      "Trainable params: 7,115,590\n",
      "Non-trainable params: 5,120\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(128, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.7642 - accuracy: 0.2384\n",
      "Epoch 00001: val_loss improved from inf to 1.75063, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 136s 77ms/step - loss: 1.7642 - accuracy: 0.2384 - val_loss: 1.7506 - val_accuracy: 0.2500\n",
      "Epoch 2/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.7463 - accuracy: 0.2486\n",
      "Epoch 00002: val_loss improved from 1.75063 to 1.71702, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 133s 76ms/step - loss: 1.7463 - accuracy: 0.2485 - val_loss: 1.7170 - val_accuracy: 0.2795\n",
      "Epoch 3/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.7099 - accuracy: 0.2753\n",
      "Epoch 00003: val_loss improved from 1.71702 to 1.69487, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 131s 74ms/step - loss: 1.7098 - accuracy: 0.2754 - val_loss: 1.6949 - val_accuracy: 0.2702\n",
      "Epoch 4/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.6510 - accuracy: 0.3074\n",
      "Epoch 00004: val_loss improved from 1.69487 to 1.63755, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 132s 75ms/step - loss: 1.6511 - accuracy: 0.3074 - val_loss: 1.6376 - val_accuracy: 0.3636\n",
      "Epoch 5/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.6027 - accuracy: 0.3399\n",
      "Epoch 00005: val_loss improved from 1.63755 to 1.56987, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 134s 76ms/step - loss: 1.6028 - accuracy: 0.3398 - val_loss: 1.5699 - val_accuracy: 0.3875\n",
      "Epoch 6/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.5332 - accuracy: 0.3786\n",
      "Epoch 00006: val_loss improved from 1.56987 to 1.55121, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 134s 76ms/step - loss: 1.5332 - accuracy: 0.3786 - val_loss: 1.5512 - val_accuracy: 0.4111\n",
      "Epoch 7/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.4812 - accuracy: 0.4036\n",
      "Epoch 00007: val_loss did not improve from 1.55121\n",
      "1767/1767 [==============================] - 132s 75ms/step - loss: 1.4812 - accuracy: 0.4036 - val_loss: 1.6438 - val_accuracy: 0.4136\n",
      "Epoch 8/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.4312 - accuracy: 0.4287\n",
      "Epoch 00008: val_loss did not improve from 1.55121\n",
      "1767/1767 [==============================] - 132s 75ms/step - loss: 1.4310 - accuracy: 0.4288 - val_loss: 1.5536 - val_accuracy: 0.4253\n",
      "Epoch 9/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.3975 - accuracy: 0.4432\n",
      "Epoch 00009: val_loss improved from 1.55121 to 1.52900, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 131s 74ms/step - loss: 1.3975 - accuracy: 0.4431 - val_loss: 1.5290 - val_accuracy: 0.4503\n",
      "Epoch 10/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.3703 - accuracy: 0.4582\n",
      "Epoch 00010: val_loss improved from 1.52900 to 1.46699, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 132s 75ms/step - loss: 1.3705 - accuracy: 0.4580 - val_loss: 1.4670 - val_accuracy: 0.4491\n",
      "Epoch 11/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.3468 - accuracy: 0.4694\n",
      "Epoch 00011: val_loss did not improve from 1.46699\n",
      "1767/1767 [==============================] - 132s 75ms/step - loss: 1.3467 - accuracy: 0.4694 - val_loss: 1.5067 - val_accuracy: 0.4832\n",
      "Epoch 12/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.3340 - accuracy: 0.4755\n",
      "Epoch 00012: val_loss did not improve from 1.46699\n",
      "1767/1767 [==============================] - 140s 79ms/step - loss: 1.3341 - accuracy: 0.4756 - val_loss: 1.4688 - val_accuracy: 0.4705\n",
      "Epoch 13/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.3065 - accuracy: 0.4861\n",
      "Epoch 00013: val_loss improved from 1.46699 to 1.42335, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 136s 77ms/step - loss: 1.3063 - accuracy: 0.4862 - val_loss: 1.4234 - val_accuracy: 0.4889\n",
      "Epoch 14/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.3034 - accuracy: 0.4902\n",
      "Epoch 00014: val_loss improved from 1.42335 to 1.40168, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 138s 78ms/step - loss: 1.3033 - accuracy: 0.4903 - val_loss: 1.4017 - val_accuracy: 0.4821\n",
      "Epoch 15/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.2856 - accuracy: 0.4988\n",
      "Epoch 00015: val_loss did not improve from 1.40168\n",
      "1767/1767 [==============================] - 134s 76ms/step - loss: 1.2856 - accuracy: 0.4988 - val_loss: 1.5046 - val_accuracy: 0.4875\n",
      "Epoch 16/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.2774 - accuracy: 0.5049\n",
      "Epoch 00016: val_loss did not improve from 1.40168\n",
      "1767/1767 [==============================] - 138s 78ms/step - loss: 1.2774 - accuracy: 0.5049 - val_loss: 1.4574 - val_accuracy: 0.4957\n",
      "Epoch 17/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.2622 - accuracy: 0.5094\n",
      "Epoch 00017: val_loss improved from 1.40168 to 1.39574, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 137s 77ms/step - loss: 1.2622 - accuracy: 0.5094 - val_loss: 1.3957 - val_accuracy: 0.4949\n",
      "Epoch 18/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.2542 - accuracy: 0.5133\n",
      "Epoch 00018: val_loss did not improve from 1.39574\n",
      "1767/1767 [==============================] - 137s 78ms/step - loss: 1.2541 - accuracy: 0.5133 - val_loss: 1.3967 - val_accuracy: 0.5091\n",
      "Epoch 19/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.2483 - accuracy: 0.5168\n",
      "Epoch 00019: val_loss improved from 1.39574 to 1.37421, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 137s 77ms/step - loss: 1.2484 - accuracy: 0.5168 - val_loss: 1.3742 - val_accuracy: 0.5239\n",
      "Epoch 20/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.2333 - accuracy: 0.5285\n",
      "Epoch 00020: val_loss did not improve from 1.37421\n",
      "1767/1767 [==============================] - 136s 77ms/step - loss: 1.2333 - accuracy: 0.5285 - val_loss: 1.4439 - val_accuracy: 0.5372\n",
      "Epoch 21/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.2239 - accuracy: 0.5290\n",
      "Epoch 00021: val_loss improved from 1.37421 to 1.33709, saving model to ../Trained Models/emotion_little_vgg_4.h5\n",
      "1767/1767 [==============================] - 138s 78ms/step - loss: 1.2238 - accuracy: 0.5290 - val_loss: 1.3371 - val_accuracy: 0.5236\n",
      "Epoch 22/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.2096 - accuracy: 0.5360\n",
      "Epoch 00022: val_loss did not improve from 1.33709\n",
      "1767/1767 [==============================] - 135s 77ms/step - loss: 1.2096 - accuracy: 0.5360 - val_loss: 1.4619 - val_accuracy: 0.5210\n",
      "Epoch 23/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.2020 - accuracy: 0.5422\n",
      "Epoch 00023: val_loss did not improve from 1.33709\n",
      "1767/1767 [==============================] - 140s 79ms/step - loss: 1.2023 - accuracy: 0.5421 - val_loss: 1.3721 - val_accuracy: 0.5222\n",
      "Epoch 24/50\n",
      "1766/1767 [============================>.] - ETA: 0s - loss: 1.1896 - accuracy: 0.5451Restoring model weights from the end of the best epoch.\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.33709\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "1767/1767 [==============================] - 136s 77ms/step - loss: 1.1901 - accuracy: 0.5450 - val_loss: 1.3485 - val_accuracy: 0.5315\n",
      "Epoch 00024: early stopping\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"../Trained Models/emotion_little_vgg_4.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "Confusion Matrix\n",
      "[[249  48  22  60 109   3]\n",
      " [111 114  23  78 151  51]\n",
      " [ 15  17 749  47  39  12]\n",
      " [100  61 143 143 128  51]\n",
      " [ 58  31  24 128 348   5]\n",
      " [ 19  86  29  23   9 250]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.45      0.51      0.48       491\n",
      "        Fear       0.32      0.22      0.26       528\n",
      "       Happy       0.76      0.85      0.80       879\n",
      "     Neutral       0.30      0.23      0.26       626\n",
      "         Sad       0.44      0.59      0.51       594\n",
      "    Surprise       0.67      0.60      0.63       416\n",
      "\n",
      "    accuracy                           0.52      3534\n",
      "   macro avg       0.49      0.50      0.49      3534\n",
      "weighted avg       0.51      0.52      0.51      3534\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "classifier = load_model('emotion_little_vgg_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "{0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test on some of validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dyada\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras_preprocessing\\image\\utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predited - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = './fer2013/validation/' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    allfaces = []   \n",
    "    rects = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "        allfaces.append(roi_gray)\n",
    "        rects.append((x,w,y,h))\n",
    "    return rects, allfaces, img\n",
    "\n",
    "img = cv2.imread(\"david.jpg\")\n",
    "rects, faces, image = face_detector(img)\n",
    "\n",
    "i = 0\n",
    "for face in faces:\n",
    "    roi = face.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "    # make a prediction on the ROI, then lookup the class\n",
    "    preds = classifier.predict(roi)[0]\n",
    "    label = class_labels[preds.argmax()]   \n",
    "\n",
    "    #Overlay our detected emotion on our pic\n",
    "    label_position = (rects[i][0] + int((rects[i][1]/2)), abs(rects[i][2] - 10))\n",
    "    i =+ 1\n",
    "    cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Emotion Detector\", image)\n",
    "cv2.imwrite('output.jpg', image) \n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try this on our webcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "out = cv2.VideoWriter('output.mp4', -1, 20.0, (640,480))\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    out.write(image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
